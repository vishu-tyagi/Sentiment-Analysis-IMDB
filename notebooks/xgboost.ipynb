{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sentiment_analysis.config import SentimentAnalysisConfig\n",
    "from sentiment_analysis.data_access import DataClass\n",
    "from sentiment_analysis.features import Features\n",
    "from sentiment_analysis.evaluation import CustomEvaluation\n",
    "from sentiment_analysis.utils.constants import (\n",
    "    TEXT,\n",
    "    TARGET,\n",
    "    ORIGINAL_TEXT,\n",
    "    SPLIT,\n",
    "    TRAIN,\n",
    "    VALID,\n",
    "    TEST\n",
    ")\n",
    "\n",
    "PARENT_PATH = Path(os.getcwd()).parent.absolute()\n",
    "FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 22:50:29,899 - sentiment_analysis.utils.utils - INFO - func:build took: 1.25 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can a director that makes such great films...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is one of the worst film adaptations of a...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an art film that was either made in 19...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For two of the funniest comedians, the movie w...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doris Day never lets a bad script get her down...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unwatchable. You can't even make it past the f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quite average even by Monogram standards, this...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is not a good movie. Too preachy in parts...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I am really shocked that a great director like...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is one of the weakest soft porn film arou...</td>\n",
       "      <td>neg</td>\n",
       "      <td>development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment        Split\n",
       "0  how can a director that makes such great films...       neg  development\n",
       "1  This is one of the worst film adaptations of a...       neg  development\n",
       "2  This is an art film that was either made in 19...       neg  development\n",
       "3  For two of the funniest comedians, the movie w...       neg  development\n",
       "4  Doris Day never lets a bad script get her down...       neg  development\n",
       "5  Unwatchable. You can't even make it past the f...       neg  development\n",
       "6  Quite average even by Monogram standards, this...       neg  development\n",
       "7  This is not a good movie. Too preachy in parts...       neg  development\n",
       "8  I am really shocked that a great director like...       neg  development\n",
       "9  This is one of the weakest soft porn film arou...       neg  development"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = SentimentAnalysisConfig()\n",
    "config.CURRENT_PATH = PARENT_PATH\n",
    "\n",
    "data = DataClass(config)\n",
    "df = data.build()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 22:51:46,253 - sentiment_analysis.utils.utils - INFO - func:clean took: 1 min and                 16.21 sec\n",
      "2022-12-04 22:52:04,997 - sentiment_analysis.utils.utils - INFO - func:fit took: 18.72 sec\n",
      "2022-12-04 22:52:21,422 - sentiment_analysis.utils.utils - INFO - func:transform took: 16.42 sec\n",
      "2022-12-04 22:52:21,424 - sentiment_analysis.utils.utils - INFO - func:fit_transform took: 35.15 sec\n",
      "2022-12-04 22:52:23,104 - sentiment_analysis.utils.utils - INFO - func:transform took: 1.68 sec\n",
      "2022-12-04 22:52:40,818 - sentiment_analysis.utils.utils - INFO - func:transform took: 17.71 sec\n",
      "2022-12-04 22:52:42,552 - sentiment_analysis.utils.utils - INFO - func:build took: 2 min and                 12.52 sec\n"
     ]
    }
   ],
   "source": [
    "features = Features()\n",
    "df_features = features.build(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Split</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>char__ 00</th>\n",
       "      <th>char__ 1</th>\n",
       "      <th>char__ 10</th>\n",
       "      <th>char__ 11</th>\n",
       "      <th>char__ 12</th>\n",
       "      <th>char__ 13</th>\n",
       "      <th>...</th>\n",
       "      <th>word__york city</th>\n",
       "      <th>word__youll see</th>\n",
       "      <th>word__young boy</th>\n",
       "      <th>word__young girl</th>\n",
       "      <th>word__young man</th>\n",
       "      <th>word__young woman</th>\n",
       "      <th>word__youre going</th>\n",
       "      <th>word__youre looking</th>\n",
       "      <th>word__youve got</th>\n",
       "      <th>word__youve seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saw movie tv afternoon cant see anyone sit pie...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>I saw this movie on t.v. this afternoon and I ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>film true historical film useful researching l...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>This film is a true and historical film. It is...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film something like sequel white zombie since ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>This film is something like a sequel of \"White...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 5004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  Split  \\\n",
       "0  saw movie tv afternoon cant see anyone sit pie...          0  train   \n",
       "1  film true historical film useful researching l...          1  train   \n",
       "2  film something like sequel white zombie since ...          0  train   \n",
       "\n",
       "                                       Original Text  char__ 00  char__ 1   \\\n",
       "0  I saw this movie on t.v. this afternoon and I ...        0.0        0.0   \n",
       "1  This film is a true and historical film. It is...        0.0        0.0   \n",
       "2  This film is something like a sequel of \"White...        0.0        0.0   \n",
       "\n",
       "   char__ 10  char__ 11  char__ 12  char__ 13  ...  word__york city  \\\n",
       "0    0.00000        0.0        0.0        0.0  ...              0.0   \n",
       "1    0.05085        0.0        0.0        0.0  ...              0.0   \n",
       "2    0.00000        0.0        0.0        0.0  ...              0.0   \n",
       "\n",
       "   word__youll see  word__young boy  word__young girl  word__young man  \\\n",
       "0              0.0              0.0               0.0              0.0   \n",
       "1              0.0              0.0               0.0              0.0   \n",
       "2              0.0              0.0               0.0              0.0   \n",
       "\n",
       "   word__young woman  word__youre going  word__youre looking  word__youve got  \\\n",
       "0                0.0                0.0                  0.0              0.0   \n",
       "1                0.0                0.0                  0.0              0.0   \n",
       "2                0.0                0.0                  0.0              0.0   \n",
       "\n",
       "   word__youve seen  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "\n",
       "[3 rows x 5004 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 5,000\n",
      "Number of training samples: 22,500\n",
      "Training set label distribution: pos:0.50, neg:0.50\n",
      "Number of validation samples: 2,500\n",
      "Validation set label distribution: pos:0.50, neg:0.50\n"
     ]
    }
   ],
   "source": [
    "train = df_features[df_features[SPLIT].isin([TRAIN])].copy()\n",
    "valid = df_features[df_features[SPLIT].isin([VALID])].copy()\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train = train[features.vectorizer.get_feature_names_out()].copy()\n",
    "X_valid = valid[features.vectorizer.get_feature_names_out()].copy()\n",
    "Y_train = train[TARGET]\n",
    "Y_valid = valid[TARGET]\n",
    "\n",
    "pos_prob_train = sum(Y_train) / len(Y_train)\n",
    "pos_prob_valid = sum(Y_train) / len(Y_train)\n",
    "print(f\"Number of features: {X_train.shape[1]:,}\")\n",
    "print(f\"Number of training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Training set label distribution: pos:{pos_prob_train:0.2f}, neg:{1-pos_prob_train:0.2f}\")\n",
    "print(f\"Number of validation samples: {X_valid.shape[0]:,}\")\n",
    "print(f\"Validation set label distribution: pos:{pos_prob_valid:0.2f}, neg:{1-pos_prob_valid:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with early stopping on validation step\n",
    "\n",
    "This will help us discover the optimal boosting rounds. Once complete, we can train on the whole train + validation set using the discovered boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-accuracy:0.69000\n",
      "[10]\tvalidation-accuracy:0.70360\n",
      "[20]\tvalidation-accuracy:0.71480\n",
      "[30]\tvalidation-accuracy:0.72880\n",
      "[40]\tvalidation-accuracy:0.73040\n",
      "[50]\tvalidation-accuracy:0.73640\n",
      "[60]\tvalidation-accuracy:0.73640\n",
      "[70]\tvalidation-accuracy:0.74000\n",
      "[80]\tvalidation-accuracy:0.74760\n",
      "[90]\tvalidation-accuracy:0.75040\n",
      "[100]\tvalidation-accuracy:0.75320\n",
      "[110]\tvalidation-accuracy:0.75440\n",
      "[120]\tvalidation-accuracy:0.75600\n",
      "[130]\tvalidation-accuracy:0.76000\n",
      "[140]\tvalidation-accuracy:0.76400\n",
      "[150]\tvalidation-accuracy:0.76520\n",
      "[160]\tvalidation-accuracy:0.76880\n",
      "[170]\tvalidation-accuracy:0.77080\n",
      "[180]\tvalidation-accuracy:0.77080\n",
      "[190]\tvalidation-accuracy:0.77360\n",
      "[200]\tvalidation-accuracy:0.77680\n",
      "[210]\tvalidation-accuracy:0.77960\n",
      "[220]\tvalidation-accuracy:0.78080\n",
      "[230]\tvalidation-accuracy:0.78120\n",
      "[240]\tvalidation-accuracy:0.78320\n",
      "[250]\tvalidation-accuracy:0.78480\n",
      "[260]\tvalidation-accuracy:0.78560\n",
      "[270]\tvalidation-accuracy:0.78760\n",
      "[280]\tvalidation-accuracy:0.78840\n",
      "[290]\tvalidation-accuracy:0.78960\n",
      "[300]\tvalidation-accuracy:0.79080\n",
      "[310]\tvalidation-accuracy:0.79200\n",
      "[320]\tvalidation-accuracy:0.79280\n",
      "[330]\tvalidation-accuracy:0.79400\n",
      "[340]\tvalidation-accuracy:0.79440\n",
      "[350]\tvalidation-accuracy:0.79560\n",
      "[360]\tvalidation-accuracy:0.79600\n",
      "[370]\tvalidation-accuracy:0.79640\n",
      "[380]\tvalidation-accuracy:0.79720\n",
      "[390]\tvalidation-accuracy:0.79840\n",
      "[400]\tvalidation-accuracy:0.79920\n",
      "[410]\tvalidation-accuracy:0.80080\n",
      "[420]\tvalidation-accuracy:0.80200\n",
      "[430]\tvalidation-accuracy:0.80400\n",
      "[440]\tvalidation-accuracy:0.80320\n",
      "[450]\tvalidation-accuracy:0.80480\n",
      "[460]\tvalidation-accuracy:0.80560\n",
      "[470]\tvalidation-accuracy:0.80640\n",
      "[480]\tvalidation-accuracy:0.80640\n",
      "[490]\tvalidation-accuracy:0.80680\n",
      "[500]\tvalidation-accuracy:0.80760\n",
      "[510]\tvalidation-accuracy:0.80800\n",
      "[520]\tvalidation-accuracy:0.80840\n",
      "[530]\tvalidation-accuracy:0.80920\n",
      "[540]\tvalidation-accuracy:0.81040\n",
      "[550]\tvalidation-accuracy:0.81280\n",
      "[560]\tvalidation-accuracy:0.81400\n",
      "[570]\tvalidation-accuracy:0.81400\n",
      "[580]\tvalidation-accuracy:0.81480\n",
      "[590]\tvalidation-accuracy:0.81640\n",
      "[600]\tvalidation-accuracy:0.81600\n",
      "[610]\tvalidation-accuracy:0.81720\n",
      "[620]\tvalidation-accuracy:0.81800\n",
      "[630]\tvalidation-accuracy:0.81680\n",
      "[640]\tvalidation-accuracy:0.81680\n",
      "[650]\tvalidation-accuracy:0.81720\n",
      "[660]\tvalidation-accuracy:0.81760\n",
      "[670]\tvalidation-accuracy:0.81800\n",
      "[680]\tvalidation-accuracy:0.81840\n",
      "[690]\tvalidation-accuracy:0.81880\n",
      "[700]\tvalidation-accuracy:0.81920\n",
      "[710]\tvalidation-accuracy:0.82000\n",
      "[720]\tvalidation-accuracy:0.82080\n",
      "[730]\tvalidation-accuracy:0.82080\n",
      "[740]\tvalidation-accuracy:0.82120\n",
      "[750]\tvalidation-accuracy:0.82200\n",
      "[760]\tvalidation-accuracy:0.82200\n",
      "[770]\tvalidation-accuracy:0.82320\n",
      "[780]\tvalidation-accuracy:0.82320\n",
      "[790]\tvalidation-accuracy:0.82440\n",
      "[800]\tvalidation-accuracy:0.82440\n",
      "[810]\tvalidation-accuracy:0.82480\n",
      "[820]\tvalidation-accuracy:0.82480\n",
      "[830]\tvalidation-accuracy:0.82440\n",
      "[840]\tvalidation-accuracy:0.82440\n",
      "[850]\tvalidation-accuracy:0.82480\n",
      "[860]\tvalidation-accuracy:0.82520\n",
      "[870]\tvalidation-accuracy:0.82680\n",
      "[880]\tvalidation-accuracy:0.82600\n",
      "[890]\tvalidation-accuracy:0.82720\n",
      "[900]\tvalidation-accuracy:0.82680\n",
      "[910]\tvalidation-accuracy:0.82680\n",
      "[920]\tvalidation-accuracy:0.82720\n",
      "[930]\tvalidation-accuracy:0.82800\n",
      "[940]\tvalidation-accuracy:0.82760\n",
      "[950]\tvalidation-accuracy:0.82760\n",
      "[960]\tvalidation-accuracy:0.82840\n",
      "[970]\tvalidation-accuracy:0.82880\n",
      "[980]\tvalidation-accuracy:0.82880\n",
      "[990]\tvalidation-accuracy:0.82880\n",
      "[999]\tvalidation-accuracy:0.82960\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for training and validation\n",
    "d_train = xgb.DMatrix(X_train, Y_train)\n",
    "d_val = xgb.DMatrix(X_valid, Y_valid)\n",
    "evals = [(d_val, \"validation\")]\n",
    "\n",
    "# Evaluation class to access custom objective and evaluation metric\n",
    "eval = CustomEvaluation()\n",
    "\n",
    "model = xgb.train(\n",
    "    params=config.XGB_PARAMETERS,\n",
    "    dtrain=d_train,\n",
    "    num_boost_round=config.XGB_NUM_BOOST_ROUND,\n",
    "    evals=evals,\n",
    "    obj=eval.binary_logistic,\n",
    "    custom_metric=eval.accuracy_eval,\n",
    "    maximize=True,\n",
    "    early_stopping_rounds=config.XGB_EARLY_STOPPING_ROUNDS,\n",
    "    verbose_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt boosting rounds after validation: 985\n"
     ]
    }
   ],
   "source": [
    "num_boost_rounds = model.best_iteration\n",
    "print(f\"Learnt boosting rounds after validation: {num_boost_rounds:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the threshold which maximizes the accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid_pred_probab = model.predict(xgb.DMatrix(X_valid))\n",
    "\n",
    "threshold = eval.threshold_discovery(y_true=Y_valid.to_numpy(), y_pred_probab=Y_valid_pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy     0.829600\n",
       "Precision    0.816295\n",
       "Recall       0.850280\n",
       "F1 Score     0.832941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid_pred = np.where(Y_valid_pred_probab > threshold, 1, 0)\n",
    "\n",
    "eval.evaluate(y_true=Y_valid.to_numpy(), y_pred=Y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy     0.833880\n",
       "Precision    0.821955\n",
       "Recall       0.852400\n",
       "F1 Score     0.836901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df_features[df_features[SPLIT].isin([TEST])].copy()\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_test = test[features.vectorizer.get_feature_names_out()].copy()\n",
    "Y_test = test[TARGET]\n",
    "\n",
    "Y_test_pred_probab = model.predict(xgb.DMatrix(X_test))\n",
    "Y_test_pred = np.where(Y_test_pred_probab > threshold, 1, 0)\n",
    "\n",
    "eval.evaluate(y_true=Y_test.to_numpy(), y_pred=Y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('sentiment-analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4147dedb0d2f4fd7bd9f9daaae1bb896b5adcf74c54646418d2fdc1ab4f35694"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
